[{"content":" Note To keep it confidential, all company-specific metrics has been replaced by public dataset. Background #üí∞ The revenue gap after a purchase #One of the effective campaigns to customers is introducing other products during the transaction. This approach has been applied in various ways such as e-commerce apps show other products through banners right when customers are doing the payment, or in minimarket, you probably found the storekeepers mentioning flash sale products to attract customers.\nüó∫Ô∏è Invisible pattern: products come in sequence # Surprisingly, some stores are smart enough to guess what the next products that the customers are morelikely to buy. For example, after purchasing a phone, storekeepers are going to offer phone case. In addition, if a customer comes to buy a case, then the next product wouldn\u0026rsquo;t be a phone, but probably other gadget accessories. Therefore, customers choose items sequentially, and this model is going find the most natural best product according to sequential behavior.\n‚öõÔ∏è NetworkX to appraoch sequential model # In this article, we are going to use NetworkX, a branch of graph theory, to explain natural behavior how customers decide purchasing an item accoring to their past behavior of choosing the others.\nThe analysis that will be discussed are the nodes, meaning the products including how frequent they are purchased, and the edge that refers to the best path of the next products. It is important to notes, in the calculation, only customers who made more than two transactions are involved to show natural transition moving from a product to another.\nüõí Implementing real scenario # The use case is retail products, covering three categories (clothes, electronic, and furniture) with total 17 disticnt products. The data has been cleansed in sql which then transformed into graph model. Therefor, for technical details, I suggest you to visit my github.\nView on GitHub üìà Graph result # The graph shows that almost all products related with each others, drawn by the scattered points that are connected. In addition, the hightlighted lines are configured to refer the most frequent transitin that is regarded as the highest occurance path of the next products. For eample, after choosing accessories, customers are most likelye purchase printers.\nOverall, there are three products that are potentially purchased repeatedly, namely phones, hankerchief and saree. Yet, the other products tend to transitioning to the other. Most transition happened in Stole to Hankercief, calculated around 3.6% of total transition.\n‚¨ÜÔ∏è Incresing the probability # To enhance probability, instead of offering one product accoridng to the graph, it\u0026rsquo;s also possible to offer other possible option by sorting the several highest possible products. The table above depicts the card that consist some options that are best to offer according each last transaction.\nüéØ Further improvements #It is important to note that the application of this model depends on what types of the business. Consequently, it is suggested to support the model using test and learn, so the impact can be more reliable.\nMoreover, this model can also be used to understand customer journey, that can be applied for helping customers finding products they want through personalized approach, such as product placement in application.\n","date":"26 December 2025","permalink":"http://localhost:1313/posts/sequential_recommender/","section":"Projects","summary":"By using sequential approach, it helps reveal niche and seasonal products, avoiding saturated offers that are commonly driven by massive transaction.","title":"Sequential Recommender System"},{"content":" Note To keep it confidential, all company-specific metrics has been replaced by public dataset. ‚ùì What does it matter? #üì¢ Understanding the scenario #It\u0026rsquo;s commonly believed that smoking increases the risk of stroke, yet it requires futher observation to validate the assumption. One of the most trusted ways to understand this phenomenon by using AB testing experiment, splitting a group to target subgroup, who is allowed to smoke, and the control one who isn\u0026rsquo;t, then comparing them afterward. On the other hand, doing such an experiment can be hardly executed due to ethical reasons. Likewise, a similar challenge also happens to the other sector like marketing, the field where I\u0026rsquo;m working. Consequently, measuring the existing data, comparing smokers and non-smokers, is an effective option to answer the hypothesis.\nüìâ The bias comes here #Here is the twist, after learning the two groups (the smokers and those who don\u0026rsquo;t), we found that the two have different physical condition such as age, weight, and other status. As a result, comparing stroke status of both groups can be bias since those difference could interfere the output\nüéØ The solution #To resolve the issue, propensity matching score, as a part of causal approach, can be implemented to match the available data of the two in the same profile, ensuring the making the decision based on apple-to-apple comparison.\nüßÆ Into Matching Algorithm #ü§ñ A brief of algorithm #The main calculation is finding every smoker\u0026rsquo;s pair. This looks easy until so many features we have to involve in the calculation. This is very similar in seeking a couple, which is getting more complex to match all the criteria. To address this, probability score of logistic regression is used, representing all features, than finding the pair using neighbor algorithm. For your information, I used sklearn neigbor finder to solve computational issues, as this is compatible for handling large amount of data.\n‚¨ÜÔ∏è the Improvement #When it comes to big data, calculating propensity score matching requires so much computational process, as the algorithm involves combination. That\u0026rsquo;s the reason why my team designed our own algorithm, despite the availibility out there. The algorithm we made approximately can handle around ~10 millions. According to my experience, it took minutes to complete calculation using M1 apple sillicon. So, it eventually depends on the computing power.\nFor detail explanation about technical details, including how to use my library and to call the syntax, please refer to my github:\nView on GitHub To find out more about the algorithm, you better watch this youtube video: üí° Insight Explaination #üî¢ The data #From the data, there are three terminologies that are important to remember, consisiting of feature, treatment, and outcome. The following table are the explanation of terminologies and description that are going to be discuss.\nTerminology Description Fields Features/Confounders Possible causes that might influence the outcome hypertension, gender, heart_disease, bmi (body mass index), avg_glucose_level, age Treatment The variable that indicates the indvidual has been treated or not smoking status Outcome The result that we want to observe stroke üñ•Ô∏è Matching # Before applying the algorithm, the data shows that both smokers and non-smokers have different profile. One of the noticable one is that the total of non-smokers are far larger than the smokers. Again, the distribution of all confounders are different, heart disease status for example, that there are 18.9% of smokers out of total heart disease sufferers, meanwhile in negative group, the smokers only makes 15.1%. The imbalance might interfere the conclusion because smokers group mostly suffering heart disease.\nCompared to data after matching process, the distribution of smokers and non-smokers are similar almost in every feature. For example, in hypertension, the ratio of smokers and non-smokers in the positive hypertension group is 50:50, and so the negative one is. As a result, measuring stroke status between the two groups can be much more reliable as both have already similar.\nüîé Evaluation # To support distribution analysis, Standard Mean Difference (SMD) is used as the indicators whether two values of distribution are similar or not. In simple explanation, smaller SMD value means that two groups are in common.\nAccording to the experiment, there are four confounders (hypertension, age, bmi, and heart_disease) decreasing SMD values, meaning that those features are getting similar after calculation. Despite an increase of glucose level and gender, this is still acceptable as only 2 out of 6 features which only experienced this.\nThe result shows that 5.3% of smoking group suffers stroke. Before matching, non-smoker group accounted for 4.8%, making 0.5% difference of percentage point with smoking group. In fact, post calculation reveals that the difference of percentage points are larger, around 1.4 percentage points, meaning that smokers are more riskier to suffer stroke\n‚õ≥Ô∏è Further Development #In the theory of causal model, more common term to evaluate result is ATT (average treatment on treated). This involves further model, instead of directly comparing the treated group with the synthetic one. The actual algorithm has already covered this, yet discussing the usage will be discussed later article.\n","date":"1 December 2025","permalink":"http://localhost:1313/posts/propensity_score_matching/","section":"Projects","summary":"The method is a brach of causal models, improving clarity in impact measurement for non-control experiments, by making artificial apple-to-apple comparable groups.","title":"Propensity Score Matching"},{"content":" Note To keep it confidential, all company-specific metrics has been replaced by public dataset. ‚ùì Background #üëç Why you can\u0026rsquo;t use LLM to decide positive or negative feedbacks. #Tagging sentiment of feedbacks can be exhausting especially dealing with large amount of feedbacks. In addition, some companies might restrict the use of AI because of potentially revealing the secrets, unless you totally trust to AI CEO\u0026rsquo;s. In another case, probably you want to train AI by creating reliable raw data. Therefore, this article is going to discuss how to annotate text sentiment by combining word-cloud and graph in python.\nüíº Real Scenario #üìΩÔ∏è IMDb (Internet Movie Database) ratings # In this case, there are around 400K feedbacks, giving comments about their experience watching various movies. Yet, these feedbacks still require further cleansing since they are extracted using machine before directly analyzing it. To resolve this issue, I used three following methods.\nMethod Description Stopwords Several words do not contain useful meaning, and they are typically used in every phase so they might interfere the most frequent words. Therefore, this step skips those words HTML syntax as these comments are generated by machine, some coding words are detected. So, this process removes them. Lemmitizer Grammatical rules make english words having so many branches, even though the meanings remain the same. For example, \u0026ldquo;ate\u0026rdquo; and \u0026ldquo;eaten\u0026rdquo; come from \u0026ldquo;eat\u0026rdquo;. Therefor, this part returns all the braches to the root. ‚òÅÔ∏è Word Frequency by WordCloud # WordCloud is a common method to understand overall feedback by analyzing the frequent words appearing in all feedbacks. The picture shows that film, movie, and one reveal many times, which are also indicated by the large font size. Focusing on positive and negative sentiment, the word \u0026ldquo;good\u0026rdquo; is identified as the fifth most frequent word (23.9 K), meanwhile the \u0026ldquo;bad\u0026rdquo; word comprises 14.6K. Therefore, initial insight is that IMDB ratings positive feedbacks compared to bad feedbacks according to the frequency.\nOn the other hand, \u0026ldquo;good\u0026rdquo; and \u0026ldquo;bad\u0026rdquo; words are not enough to conclude the sentiment as the context might be different, especially when the feedbacks containing both words at the same time. Likewise, those words probably refer to other aspects not directly on the movie. For instance, the feedback of \u0026ldquo;Even though the quality of the video was bad, I\u0026rsquo;d say it is a good movie as the actress \u0026hellip;..\u0026rdquo; shows that \u0026ldquo;bad\u0026rdquo; does not refer to the movie, which can be misleading in representing the sentiment. Therefore, analyzing the relation between words is required to resolve this problem.\nüï∏Ô∏è Graph Analysis # Looking at the intersection, there are about 5.1K feedbacks containing both \u0026ldquo;good\u0026rdquo; and \u0026ldquo;bad\u0026rdquo; in the sentences. These feedbacks are going to be analyzed further by analyzing the correlation among other words.\nBefore connecting the words, it is important to note the weight rule is applied in this analysis. The weight gives a level for every connection, indicating how close enough two words appeared in a sentences. In this case, high weight assumes that \u0026ldquo;good\u0026rdquo; or \u0026ldquo;bad\u0026rdquo; close enough to the \u0026ldquo;movie\u0026rdquo;, so it can be assumed to decide the final sentiment.\nAccording to the graph, the connection between nodes indicates that the words are in the same feedback, and the width of lines represets the weight rule. The result shows for the feedbacks containing both words, connection between movie and bad is strong, meaning that the feedbacks are more likely to have negative sentiment over the positive ones. Nevertheless, this would be better to return this weight to individual feedbacks for annotating purposes. In addition, the sample of feedbacks are also provided to quick check the validation like in the picture below.\n‚öôÔ∏è Further Improvement #Further development should be considered especially handling negation phrases like ‚Äúnot a good movie‚Äù, and synonyms like film and movie. Such examples demonstrate that the despite strong relation but can‚Äôt affect the actual meanings whether a feedback is positive or negative. To find out more about technical details, you can explore in my github below\nView on GitHub ","date":"26 December 2025","permalink":"http://localhost:1313/posts/sentiment_annotation/","section":"Projects","summary":"WordCloud and Graph are used mainly in this analysis in order to understand word frequency and how strong the relation among them.","title":"Sentiment Annotation"},{"content":"Updated Soon #","date":"7 December 2025","permalink":"http://localhost:1313/posts/digital_dna/","section":"Projects","summary":"The algorithm that helps to find artificial control group for apple-to-apple comparison, avoiding bias in measurement impacts","title":"Digital DNA"},{"content":" Best Projects #These are examples of what I made in my company to solve real problems through data approaches.\nSequential Recommender SystemDeveloped the Digital Recommendation increasing around ~150% revenue compared to Collaborative Filtering model.By using sequential approach, it helps reveal niche and seasonal products, avoiding saturated offers that are commonly driven by massive transaction.View project ‚Üí Propensity Score MatchingImproved Above-the-Line (ATL) campaign measurement, identifying approximately ~$300K/month at-risk-revenue.The method is a brach of causal models, improving clarity in impact measurement for non-control experiments, by making artificial apple-to-apple comparable groups.View project ‚Üí Sentiment AnnotationAccelerated textual data to determine sentiment tagging. This has potential for analyzing confidential information that AI could not handle or preparing training data.WordCloud and Graph are used mainly in this analysis in order to understand word frequency and how strong the relation among them.View project ‚Üí View more -\u0026gt;\n","date":null,"permalink":"http://localhost:1313/","section":"Azka Rohbiya","summary":"","title":"Azka Rohbiya"},{"content":" Let‚Äôs start a conversation Email or LinkedIn works‚Äîwhatever‚Äôs easiest. Email LinkedIn ","date":"27 December 2025","permalink":"http://localhost:1313/contact/","section":"Azka Rohbiya","summary":"","title":"Contact"},{"content":"üë§ About Me #I have over 5 years of experience working with data works, focusing in predictive modeling, reporting, marketing analytics, and data pitch decks. Moreover, I\u0026rsquo;m really into communicative insights, making scattered numbers to be concise charts, so ultimately they can be readily grasped by any team with various backgrounds. Now, I continuously seek more journey to help more teams solving their challenges through sophisticated analysis.\nüî¨ Field of Interests # In analytics, I\u0026rsquo;m really into grasping the domain knowledge first over the technical tools, focusing the clarity in order to address the real problems. Therefore, my experience ranges in these fields:\nMarketing (Customer Journey and Behaviors) Science (Mostly in Physics and Chemistry) Environment (I\u0026rsquo;m into environmental strategies to enchance the quality of live) Engineering (Interested in renewable energy, autopilot cars) Economic (Stock Analysis, Cryptocurrency) üìù Resume #To see more about professional journey, please refer to my resume!\nResume ","date":"27 December 2025","permalink":"http://localhost:1313/about/","section":"Azka Rohbiya","summary":"","title":"Introduction"},{"content":"üíº Professional Background # Sep\u0026#39;22 ‚Äî PresentData Analyst ¬∑ TelkomselOne of the best projects was developing recommendation system, our team developed the algorithm to assist our customers, with the total target around ~80 Mn, decide their best internet package choices. The result was significant, contributing around ~USD 256 million in 2024.*About the Company Jul\u0026#39;22 - Sep\u0026#39;22Data Analyst ¬∑ JD.id (Jingdong Group)Created three web scrapers for internal company websites with the goals of automating data uploads to a data warehouse and validating data against real-time data. Libraries like Scrappy, Selenium, and Google Sheet API were used in this.About the Company Oct\u0026#39;21 - Jun\u0026#39;22Data Analyst ¬∑ Kinarya Alihdaya MandiriFocusing on Games and Video campaings, successfully uplifted revenue around ~1.5% permonth. Involved in developing algorithm of Propensity Matching Score in order to measure campaign impact on Above-The-Line campaignAbout the Company Sep\u0026#39;20 - Sep\u0026#39;21Flavorist and Production ¬∑ Foom Lab GlobalInitiated digitalisation in production reports, automating tons of google sheets based forms using Googls Apps Script. The implemetation improved producation planning, indicated by growing number production by around 133% within 6 monthsAbout the Company üéì Educational Background # 2016 - 2020Universitas PertaminaGPA 3.8\nThesis Computational Chemistry, Molecular Dynamics Simulation in the Delignification Process by Low Transition Temperature Mixtures (LTTM) Solvents Using the GROMACS Application (Linux-based Apps). Read more \u0026raquo; Publication Clove Oil Extraction by Steam Distillation and Utilization of Clove Buds Waste as Potential Candidate for Eco-Friendly Packaging Read more\u0026raquo; About the University 2019Universiti Teknologi PetronasMalaysia, the Student Exchange ProgramAbout the University üì¢ Language Proficiency # IELTSScore 6.5 (CERF B2)\nIssued by IDP IELTS Date 5 Dec 2025\nPreview Certificate üß† Bootcamp # Tracks Date Issued By Safe AI Practices Sep 2025 LinkedIn Using Generative AI Ethically at Work Aug 2025 LinkedIn Product Management : Customer Development July 2025 LinkedIn Business Ethics July 2025 LinkedIn Data Analyst with Python Track 28 Apr 2022 Datacamp Data Analyst with SQL Server Track 28 Feb 2022 Datacamp Data Engineer in Python 23 Jan 2022 Datacamp Python Programmer 7 Dec 2021 Datacamp ata Science 4 July 2021 Sololearn Machine Learning 11 July 2021 Sololearn SQL 17 July 2021 Sololearn üéØ Certification # Certificate Date Issued By SQL Advanced 3 May 2022 Hackerrank Python 18 Oct 2021 Hackerrank üìù Resume # Resume ","date":"27 December 2025","permalink":"http://localhost:1313/journey/","section":"Azka Rohbiya","summary":"","title":"Journey"},{"content":"This section contains all my current projects, know namsaying\n","date":null,"permalink":"http://localhost:1313/posts/","section":"Projects","summary":"","title":"Projects"},{"content":"","date":null,"permalink":"http://localhost:1313/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":null,"permalink":"http://localhost:1313/tags/","section":"Tags","summary":"","title":"Tags"}]